{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "import re\n",
    "\n",
    "API_KEY = \"01946b8515c545443cdcd262a884a88dab1be54962aad37f4f93c3420cc49844\"\n",
    "headers = {\"X-API-Key\": API_KEY, 'Accept-Charset': 'utf-8'}\n",
    "\n",
    "\n",
    "\n",
    "def extract_data (url): \n",
    "\n",
    "    params = {\n",
    "        \"limit\": 1000,\n",
    "        \"page\": 1\n",
    "    }\n",
    "\n",
    "    all_results = []\n",
    "\n",
    "    while True:\n",
    "        response = requests.get(url, headers=headers, params=params)\n",
    "        if response.status_code != 200:\n",
    "            print(\"Error:\", response.status_code, response.text)\n",
    "            break\n",
    "\n",
    "        data = response.json()\n",
    "        results = data.get('results', [])\n",
    "        all_results.extend(results)\n",
    "\n",
    "        if len(results) < params['limit']:\n",
    "            break\n",
    "\n",
    "        params['page'] += 1\n",
    "\n",
    "        if params['page'] > 1:  # Stop after fetching 3 pages\n",
    "            break\n",
    "            \n",
    "\n",
    "    print(f'encoding: {response.encoding}')\n",
    "    print(f\"Total records retrieved: {len(all_results)}\")\n",
    "    print(f'extract_output {all_results[:3]}')\n",
    "    \n",
    "\n",
    "    return all_results \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting worldwide sensor locations\n",
    "url= \"https://api.openaq.org/v3/locations\"\n",
    "locations_raw = extract_data(url)\n",
    "df_locations_raw = pd.DataFrame(locations_raw)\n",
    "df_locations_raw.rename(columns={'id': 'location_id', 'name': 'location_name'}, inplace=True)\n",
    "print(f'df_locations_raw output:\\n {df_locations_raw.head(3)}')\n",
    "df_locations_raw.to_csv('pollution_data/1_locations_raw.csv', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Unpack countries and coordinates\n",
    "df_country = df_locations_raw['country'].apply(pd.Series)\n",
    "df_coordinates = df_locations_raw['coordinates'].apply(pd.Series)\n",
    "df_country.columns= ['country_id', 'country_code', 'country_name']\n",
    "df_coordinates.columns= ['latitude', 'longitude']\n",
    "\n",
    "#concatenate with location\n",
    "df_locations_unpacked=pd.concat([df_locations_raw,df_country, df_coordinates],axis=1)\n",
    "print(f'df_locations_unpacked output:\\n {df_locations_unpacked[:3]}')\n",
    "df_locations_unpacked.to_csv('pollution_data/2_locations_unpacked.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep only useful columns\n",
    "df_locations_unpacked.drop(['owner', 'provider', 'isMobile', 'isMonitor', 'instruments', 'licenses', 'bounds', 'distance', 'country', 'coordinates'], axis=1, inplace=True,errors='ignore')\n",
    "df_locations_clean = df_locations_unpacked\n",
    "print(f'df_locations_clean output:\\n{df_locations_clean.head(3)}')\n",
    "df_locations_clean.to_csv('pollution_data/3_locations_raw_clean.csv', index= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unpack  the 'sensors' list of sensor vertically\n",
    "df_exploded = df_locations_clean.explode(\"sensors\")\n",
    "\n",
    "\n",
    "# flatten (normalized)  the nested dictionary structure)\n",
    "sensors_normalized = pd.json_normalize(df_exploded['sensors'])\n",
    "\n",
    "\n",
    "# Concatenate normalized sensor data exploded DataFrame\n",
    "df_exploded = pd.concat([df_exploded.reset_index(drop=True), sensors_normalized], axis=1)\n",
    "\n",
    "# clean\n",
    "df_exploded[\"sensor_id\"] = df_exploded[\"id\"]\n",
    "df_exploded[\"sensor_name\"] = df_exploded[\"parameter.name\"]\n",
    "df_exploded[\"parameter_id\"] = df_exploded[\"parameter.id\"]\n",
    "df_exploded[\"units\"] = df_exploded[\"parameter.units\"]\n",
    "df_exploded[\"parameter_name\"] = df_exploded[\"parameter.displayName\"]\n",
    "df_exploded.drop(['sensors', 'id', 'name', 'parameter.id' ,'parameter.name', 'parameter.units', 'parameter.displayName'], axis=1, inplace=True)\n",
    "df_location_final = df_exploded\n",
    "\n",
    "# Print the first 3 rows to verify the output\n",
    "print(df_location_final.head(3))\n",
    "\n",
    "# Save the final DataFrame to a CSV file\n",
    "df_location_final.to_csv('pollution_data/4_locations_final.csv', index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2707\n",
      "url_list ['https://api.openaq.org/v3/sensors/6/days/monthly', 'https://api.openaq.org/v3/sensors/5/days/monthly', 'https://api.openaq.org/v3/sensors/7/days/monthly']\n"
     ]
    }
   ],
   "source": [
    "# generate URLs list\n",
    "urls_list = [f\"https://api.openaq.org/v3/sensors/{sensor_id}/days/monthly\" for sensor_id in df_exploded['sensor_id']]\n",
    "\n",
    "print(len(urls_list))\n",
    "print(f'url_list {urls_list[:3]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoding: utf-8\n",
      "Total records retrieved: 0\n",
      "extract_output []\n",
      "encoding: utf-8\n",
      "Total records retrieved: 0\n",
      "extract_output []\n",
      "encoding: utf-8\n",
      "Total records retrieved: 0\n",
      "extract_output []\n",
      "encoding: utf-8\n",
      "Total records retrieved: 0\n",
      "extract_output []\n",
      "encoding: utf-8\n",
      "Total records retrieved: 0\n",
      "extract_output []\n",
      "encoding: utf-8\n",
      "Total records retrieved: 0\n",
      "extract_output []\n",
      "encoding: utf-8\n",
      "Total records retrieved: 0\n",
      "extract_output []\n",
      "encoding: utf-8\n",
      "Total records retrieved: 0\n",
      "extract_output []\n",
      "encoding: utf-8\n",
      "Total records retrieved: 0\n",
      "extract_output []\n",
      "encoding: utf-8\n",
      "Total records retrieved: 0\n",
      "extract_output []\n",
      "encoding: utf-8\n",
      "Total records retrieved: 0\n",
      "extract_output []\n",
      "encoding: utf-8\n",
      "Total records retrieved: 0\n",
      "extract_output []\n",
      "encoding: utf-8\n",
      "Total records retrieved: 0\n",
      "extract_output []\n",
      "encoding: utf-8\n",
      "Total records retrieved: 0\n",
      "extract_output []\n",
      "encoding: utf-8\n",
      "Total records retrieved: 0\n",
      "extract_output []\n",
      "encoding: utf-8\n",
      "Total records retrieved: 0\n",
      "extract_output []\n",
      "encoding: utf-8\n",
      "Total records retrieved: 0\n",
      "extract_output []\n",
      "encoding: utf-8\n",
      "Total records retrieved: 0\n",
      "extract_output []\n",
      "encoding: utf-8\n",
      "Total records retrieved: 0\n",
      "extract_output []\n",
      "encoding: utf-8\n",
      "Total records retrieved: 15\n",
      "extract_output [{'value': 175.0, 'flagInfo': {'hasFlags': False}, 'parameter': {'id': 5, 'name': 'no2', 'units': 'µg/m³', 'displayName': None}, 'period': {'label': '1 month', 'interval': '1 month', 'datetimeFrom': {'utc': '2016-10-31T18:30:00Z', 'local': '2016-11-01T00:00:00+05:30'}, 'datetimeTo': {'utc': '2016-11-30T18:30:00Z', 'local': '2016-12-01T00:00:00+05:30'}}, 'coordinates': None, 'summary': {'min': 6.8416666984558105, 'q02': 7.978478975296021, 'q25': 115.54253768920898, 'median': 178.50955200195312, 'q75': 221.87005615234375, 'q98': 346.29432434082037, 'max': 376.9614562988281, 'avg': 175.2774213211877, 'sd': 85.01893659641206}, 'coverage': {'expectedCount': 30, 'expectedInterval': '720:00:00', 'observedCount': 28, 'observedInterval': '672:00:00', 'percentComplete': 93.0, 'percentCoverage': 93.0, 'datetimeFrom': {'utc': '2016-11-02T18:30:00Z', 'local': '2016-11-03T00:00:00+05:30'}, 'datetimeTo': {'utc': '2016-11-30T18:30:00Z', 'local': '2016-12-01T00:00:00+05:30'}}}, {'value': 57.9, 'flagInfo': {'hasFlags': False}, 'parameter': {'id': 5, 'name': 'no2', 'units': 'µg/m³', 'displayName': None}, 'period': {'label': '1 month', 'interval': '1 month', 'datetimeFrom': {'utc': '2016-11-30T18:30:00Z', 'local': '2016-12-01T00:00:00+05:30'}, 'datetimeTo': {'utc': '2016-12-31T18:30:00Z', 'local': '2017-01-01T00:00:00+05:30'}}, 'coordinates': None, 'summary': {'min': 0.06666667014360428, 'q02': 0.11916666701436043, 'q25': 25.21500062942505, 'median': 26.130966186523438, 'q75': 55.094642639160156, 'q98': 205.307336730957, 'max': 214.69375610351562, 'avg': 57.8951561671089, 'sd': 68.28286310609622}, 'coverage': {'expectedCount': 31, 'expectedInterval': '744:00:00', 'observedCount': 22, 'observedInterval': '528:00:00', 'percentComplete': 71.0, 'percentCoverage': 71.0, 'datetimeFrom': {'utc': '2016-11-30T18:30:00Z', 'local': '2016-12-01T00:00:00+05:30'}, 'datetimeTo': {'utc': '2016-12-23T18:30:00Z', 'local': '2016-12-24T00:00:00+05:30'}}}, {'value': 23.6, 'flagInfo': {'hasFlags': False}, 'parameter': {'id': 5, 'name': 'no2', 'units': 'µg/m³', 'displayName': None}, 'period': {'label': '1 month', 'interval': '1 month', 'datetimeFrom': {'utc': '2017-01-31T18:30:00Z', 'local': '2017-02-01T00:00:00+05:30'}, 'datetimeTo': {'utc': '2017-02-28T18:30:00Z', 'local': '2017-03-01T00:00:00+05:30'}}, 'coordinates': None, 'summary': {'min': 16.956817626953125, 'q02': 17.077537536621094, 'q25': 18.106138229370117, 'median': 23.008333206176758, 'q75': 28.158333778381348, 'q98': 33.499166107177736, 'max': 33.72083282470703, 'avg': 23.564390876076438, 'sd': 6.391162217524139}, 'coverage': {'expectedCount': 28, 'expectedInterval': '672:00:00', 'observedCount': 11, 'observedInterval': '264:00:00', 'percentComplete': 39.0, 'percentCoverage': 39.0, 'datetimeFrom': {'utc': '2017-02-17T18:30:00Z', 'local': '2017-02-18T00:00:00+05:30'}, 'datetimeTo': {'utc': '2017-02-28T18:30:00Z', 'local': '2017-03-01T00:00:00+05:30'}}}]\n",
      "encoding: utf-8\n",
      "Total records retrieved: 0\n",
      "extract_output []\n",
      "encoding: utf-8\n",
      "Total records retrieved: 15\n",
      "extract_output [{'value': 335.0, 'flagInfo': {'hasFlags': False}, 'parameter': {'id': 2, 'name': 'pm25', 'units': 'µg/m³', 'displayName': None}, 'period': {'label': '1 month', 'interval': '1 month', 'datetimeFrom': {'utc': '2016-10-31T18:30:00Z', 'local': '2016-11-01T00:00:00+05:30'}, 'datetimeTo': {'utc': '2016-11-30T18:30:00Z', 'local': '2016-12-01T00:00:00+05:30'}}, 'coordinates': None, 'summary': {'min': 36.349998474121094, 'q02': 78.38999969482421, 'q25': 221.8982696533203, 'median': 304.6458435058594, 'q75': 401.1673889160156, 'q98': 887.5113159179688, 'max': 916.0499877929688, 'avg': 335.39711181640627, 'sd': 194.35812274690693}, 'coverage': {'expectedCount': 30, 'expectedInterval': '720:00:00', 'observedCount': 25, 'observedInterval': '600:00:00', 'percentComplete': 83.0, 'percentCoverage': 83.0, 'datetimeFrom': {'utc': '2016-11-02T18:30:00Z', 'local': '2016-11-03T00:00:00+05:30'}, 'datetimeTo': {'utc': '2016-11-30T18:30:00Z', 'local': '2016-12-01T00:00:00+05:30'}}}, {'value': 285.0, 'flagInfo': {'hasFlags': False}, 'parameter': {'id': 2, 'name': 'pm25', 'units': 'µg/m³', 'displayName': None}, 'period': {'label': '1 month', 'interval': '1 month', 'datetimeFrom': {'utc': '2016-11-30T18:30:00Z', 'local': '2016-12-01T00:00:00+05:30'}, 'datetimeTo': {'utc': '2016-12-31T18:30:00Z', 'local': '2017-01-01T00:00:00+05:30'}}, 'coordinates': None, 'summary': {'min': 153.0, 'q02': 167.33882629394532, 'q25': 241.90135192871094, 'median': 275.3125, 'q75': 317.9130401611328, 'q98': 442.04285522460935, 'max': 445.5, 'avg': 284.97629912003225, 'sd': 74.55253274452465}, 'coverage': {'expectedCount': 31, 'expectedInterval': '744:00:00', 'observedCount': 23, 'observedInterval': '552:00:00', 'percentComplete': 74.0, 'percentCoverage': 74.0, 'datetimeFrom': {'utc': '2016-11-30T18:30:00Z', 'local': '2016-12-01T00:00:00+05:30'}, 'datetimeTo': {'utc': '2016-12-23T18:30:00Z', 'local': '2016-12-24T00:00:00+05:30'}}}, {'value': 131.0, 'flagInfo': {'hasFlags': False}, 'parameter': {'id': 2, 'name': 'pm25', 'units': 'µg/m³', 'displayName': None}, 'period': {'label': '1 month', 'interval': '1 month', 'datetimeFrom': {'utc': '2017-01-31T18:30:00Z', 'local': '2017-02-01T00:00:00+05:30'}, 'datetimeTo': {'utc': '2017-02-28T18:30:00Z', 'local': '2017-03-01T00:00:00+05:30'}}, 'coordinates': None, 'summary': {'min': 80.56818389892578, 'q02': 82.51925354003906, 'q25': 106.24053192138672, 'median': 130.6041717529297, 'q75': 156.14583587646484, 'q98': 176.0, 'max': 176.0, 'avg': 130.73028287020597, 'sd': 33.1379332469513}, 'coverage': {'expectedCount': 28, 'expectedInterval': '672:00:00', 'observedCount': 11, 'observedInterval': '264:00:00', 'percentComplete': 39.0, 'percentCoverage': 39.0, 'datetimeFrom': {'utc': '2017-02-17T18:30:00Z', 'local': '2017-02-18T00:00:00+05:30'}, 'datetimeTo': {'utc': '2017-02-28T18:30:00Z', 'local': '2017-03-01T00:00:00+05:30'}}}]\n",
      "encoding: utf-8\n",
      "Total records retrieved: 0\n",
      "extract_output []\n",
      "encoding: utf-8\n",
      "Total records retrieved: 0\n",
      "extract_output []\n",
      "encoding: utf-8\n",
      "Total records retrieved: 0\n",
      "extract_output []\n",
      "encoding: utf-8\n",
      "Total records retrieved: 0\n",
      "extract_output []\n",
      "encoding: utf-8\n",
      "Total records retrieved: 0\n",
      "extract_output []\n",
      "encoding: utf-8\n",
      "Total records retrieved: 0\n",
      "extract_output []\n",
      "encoding: utf-8\n",
      "Total records retrieved: 0\n",
      "extract_output []\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Initialize an empty DataFrame to collect all data\n",
    "data_df = pd.DataFrame()\n",
    "\n",
    "for url in urls_list[:50]: \n",
    "    sensor_id = int(url.split(\"/\")[-3])\n",
    "    sensors_data = extract_data(url)\n",
    "    \n",
    "    # Create a list to hold records for the current URL\n",
    "    data = []\n",
    "    \n",
    "    # Iterate through the extracted data and append sensor_id to each record\n",
    "    for record in sensors_data:\n",
    "        record[\"sensor_id\"] = sensor_id\n",
    "        data.append(record)\n",
    "    \n",
    "    # Convert the list to a DataFrame\n",
    "    data_df_url = pd.DataFrame(data)\n",
    "    \n",
    "    # Append the current URL's data to the main DataFrame\n",
    "    data_df = pd.concat([data_df, data_df_url], ignore_index=True)\n",
    "\n",
    "# Unpack the 'summary' column into separate columns (only if it exists)\n",
    "if 'summary' in data_df.columns:\n",
    "    summary_df = pd.json_normalize(data_df['summary'])\n",
    "    data_df = pd.concat([data_df, summary_df], axis=1).drop('summary', axis=1)\n",
    "\n",
    "# Safely extract 'local' datetime from 'period' column (only if it exists)\n",
    "if 'period' in data_df.columns:\n",
    "    data_df['year'] = data_df['period'].apply(\n",
    "        lambda x: pd.to_datetime(x['datetimeFrom']['local']).strftime('%Y') if isinstance(x, dict) and 'datetimeFrom' in x else None\n",
    "    )\n",
    "    data_df['month'] = data_df['period'].apply(\n",
    "        lambda x: pd.to_datetime(x['datetimeFrom']['local']).strftime('%m') if isinstance(x, dict) and 'datetimeFrom' in x else None\n",
    "    )\n",
    "\n",
    "    # Display output\n",
    "    print(f'data df output (first 3 rows):\\n{data_df.head(3)}')\n",
    "    print(f'data df output length: {len(data_df)}')\n",
    "\n",
    "else:\n",
    "    print(\"The 'period' column is missing in the DataFrame.\")\n",
    "\n",
    "# Clean the data by dropping unnecessary columns\n",
    "data_df.drop(['flagInfo', 'period', 'parameter', 'coordinates'], axis=1, inplace=True, errors='ignore')\n",
    "\n",
    "# Save the cleaned DataFrame to CSV\n",
    "data_df.to_csv('pollution_data/5_sensor_data.csv', index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "to do : unpack coverage in last file/DF 5_sensor_data.csv abve json.normalize\n",
    "kill les index ? > a voir apres import ? \n",
    "revoir tous les noms et commentaires\n",
    "faire tourner big \n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "# generate sensor data  dataframe\n",
    "\n",
    "data_results = []\n",
    "sensor_ids = []\n",
    "for url in urls_list[:50] : \n",
    "    sensor_id = int(url.split(\"/\")[-3])\n",
    "    sensors_data = extract_data(url)\n",
    "    for record in sensors_data:\n",
    "        record[\"sensor_id\"] = sensor_id\n",
    "        data_results.append(record)\n",
    "    \n",
    "    data_df = pd.DataFrame(data_results)\n",
    "    \n",
    "    \n",
    "    #unpack the summary column\n",
    "    for keys, values in data_df['summary'].items():\n",
    "        data_df[keys] = values\n",
    "    \n",
    "    \n",
    "    \n",
    "    if 'period' in data_df.columns:\n",
    "    # Safely extract 'local' datetime\n",
    "        data_df['year'] = data_df['period'].apply(\n",
    "            lambda x: pd.to_datetime(x['datetimeFrom']['local']).strftime('%Y') if isinstance(x, dict) and 'datetimeFrom' in x else None\n",
    "        )\n",
    "        data_df['month'] = data_df['period'].apply(\n",
    "            lambda x: pd.to_datetime(x['datetimeFrom']['local']).strftime('%m') if isinstance(x, dict) and 'datetimeFrom' in x else None\n",
    "        )\n",
    "\n",
    "        # Display output\n",
    "        print(f'data df output:{data_df[:3]}')\n",
    "        print(f'data df output length:{len(data_df)}')\n",
    "\n",
    "    \n",
    "    else:\n",
    "        print(\"The 'period' column is missing in the DataFrame.\")\n",
    "\n",
    "# clean the data\n",
    "data_df.drop(['flaginfo', 'parameter', 'period', 'coordinates', 'summary'], axis=1, inplace=True, errors='ignore')\n",
    "# Save to CSV\n",
    "data_df.to_csv('pollution_data/5_sensor_data.csv', index=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# get the sensor ID list  along with the location id\n",
    "\n",
    "location_sensor_pairs = [\n",
    "    (location[\"id\"], sensor[\"id\"], location[\"coordinates\"][\"latitude\"], location[\"coordinates\"][\"longitude\"])\n",
    "    for location in locations_data\n",
    "    for sensor in location[\"sensors\"]\n",
    "]\n",
    "\n",
    "# Convert to DataFrame\n",
    "df_locations_sensors_pairs = pd.DataFrame(\n",
    "    location_sensor_pairs,\n",
    "    columns=[\"location_id\", \"sensor_id\", \"location_latitude\", \"location_longitude\"]\n",
    ")\n",
    "\n",
    "# Display first 3 rows\n",
    "print(f'locations_sensors_pairs_list output : {location_sensor_pairs[:4]}')\n",
    "print(f'locations_sensors_pairs_df output :\\n{df_locations_sensors_pairs.head(4)}')\n",
    "\n",
    "# Save to CSV\n",
    "df_locations_sensors_pairs.to_csv(\"pollution_data/locations_sensore_list.csv\", index=False)\n",
    "\n",
    "\n",
    "print(f'locations_sensors_pairs_list output :  {location_sensor_pairs[:3]}')\n",
    "df_locations_sensors_pairs = pd.DataFrame(location_sensor_pairs, columns=[\"location_id\", \"sensor_id\", 'location_latitude', 'location_longitude'])\n",
    "print(f'locations_sensors_pairs_df output :  {df_locations_sensors_pairs[:3]}')\n",
    "df_locations_sensors_pairs.to_csv(\"pollution_data/locations_sensore_list.csv\", index=False)\n",
    "\n",
    "  # generate the location id list and dataframe \n",
    "sensor_id_list= [location[0] for location in  location_sensor_pairs]\n",
    "print(sensor_id_list[:3])\n",
    "sensor_id_df = pd.DataFrame(sensor_id_list)\n",
    "print(f'sensor_id_df {sensor_id_df.head(3)}')\n",
    "\n",
    "#conacetenate sensor data with location ID\n",
    "final_df = pd.concat([sensor_id_df, data_df], axis=1)\n",
    "print (f'final df:{final_df[:3]}')\n",
    "print (f'final df legnth:{len(final_df)}')'''\n",
    "\n",
    "\n",
    "'''url = 'https://api.openaq.org/v3/sensors/4217/days/monthly'\n",
    "all_results = extract_data (url)\n",
    "test_df = pd.DataFrame(all_results)\n",
    "print(test_df.head(3))\n",
    "test_df.to_csv('pollution_data/5_test_sensor.csv', index= True)'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#url = \"https://api.openaq.org/v3/sensors/3917/days/monthly\"\n",
    "#url_1 = \"https://api.openaq.org/v3/sensors/3917\"\n",
    "#url_2 = \"https://api.openaq.org/v3/sensors/3917/measurements\"\n",
    "#url = \"https://api.openaq.org/v3/locations?bbox=-10.5,36,9.6,51.1\"\n",
    "#url = \"https://api.openaq.org/v3/sensors/3917/days/monthly\"\n",
    "#url = \"https://api.openaq.org/v3/sensors/3917/years\"\n",
    "#url = \"https://api.openaq.org/v3/sensors/3917/days\"\n",
    "#url = \"https://api.openaq.org/v3/sensors/3917/measurements\"\n",
    "#url = \"https://api.openaq.org/v3/parameters\"\n",
    "#url = \"https://api.openaq.org/v3/locations\"\n",
    "#url = \"https://api.openaq.org/v3/countries\"\n",
    "#urls_list = [f\"https://api.openaq.org/v3/sensors/{sensor_id}/measurements\" for sensor_id in df_exploded['sensor_id']]\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project_X1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
